# multidim-optimization

A Python application for finding the minimum of multivariable functions. This project implements classical gradient-based optimization algorithms for multidimensional problems, supporting both first-order (steepest descent) and second-order (Newton) methods, with precise one-dimensional line search.

## ğŸ“‹ Features

This application implements the following numerical methods:

1. **Steepest Descent (Gradient Descent)**
   - Direction of search: negative gradient `-âˆ‡f(x)`
   - Step size determined via accurate one-dimensional search along the direction

2. **Newton's Method**
   - Uses second-order information (Hessian matrix) for quadratic convergence near the minimum
   - Direction: `-Hâ»Â¹âˆ‡f(x)`

3. **Line Search (for Steepest Descent)**
   - **Fibonacci Search**: Efficient derivative-free method for finding optimal step size Î» in one-dimensional subproblem

4. **Graphical User Interface**
   - User-friendly input of arbitrary multivariable functions (e.g., `(1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2` â€” Rosenbrock function)
   - Selection of optimization method and parameters (precision Îµ, max iterations)
   - Display of results: minimum point, function value, number of iterations
   - Visualization of convergence (distance to optimal point vs. iteration)

## ğŸ“‚ Project Structure

```text
multidim-optimization/
â”œâ”€â”€ README.md                    # Project documentation
â”œâ”€â”€ main.py                      # Application entry point
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ line_searchers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ fibonacci_method.py  # Fibonacci search for line search
â”‚   â”œâ”€â”€ newton_method.py         # Newton's method implementation
â”‚   â””â”€â”€ steepest_descent.py      # Steepest descent with line search
â”œâ”€â”€ gui/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app_window.py            # Main window
â”‚   â”œâ”€â”€ input_widget.py          # Input configuration panel
â”‚   â””â”€â”€ result_widget.py         # Results display and plot
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ constants.py             # Constants, enums, colors, default values
    â”œâ”€â”€ containers.py            # Dataclasses: OptimizationProblem, OptimizationResult
    â””â”€â”€ ...                      # Interfaces, UI helpers, styling
```

## ğŸš€ Getting Started

### Prerequisites

- **Python 3.8+**
- Required libraries (listed in `requirements.txt`): PyQt6, NumPy, Matplotlib

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/deyme17/multidim-optimization.git
   cd multidim-optimization
   ```

2. **Install dependencies:**
    ```bash
    install -r requirements.txt
    ```

## Usage
**Run the main script to launch the GUI application**
    ```bash 
    main.py
    ```
## ğŸ§  Theory & Methods

### 1. Steepest Descent
**Purpose:** First-order method using only gradient information.  

**How it works:**
- At each iteration, move in the direction of steepest decrease: `p_k = -âˆ‡f(x_k)`
- Find optimal step size Î» by minimizing the one-dimensional function `Ï†(Î») = f(x_k + Î» Â· p_k)`
- Uses Fibonacci search for accurate and efficient line search

### 2. Newton's Method
**Purpose:** Second-order method with quadratic convergence near the minimum.  

**How it works:**
- Uses Taylor expansion up to second order
- Search direction: `p_k = -Hâ»Â¹(x_k) âˆ‡f(x_k)`, where H is the Hessian matrix
- Performs full step (Î» = 1) assuming local quadratic approximation is good

### 3. Fibonacci Line Search
**Purpose:** Find optimal step size in one-dimensional subproblem without derivatives.  

**How it works:**
- Uses Fibonacci sequence to systematically narrow the uncertainty interval
- Guarantees maximal reduction of interval for a fixed number of function evaluations
- Highly efficient for unimodal functions (which is typical along the search direction)

## ğŸ¯ Example Usage

- Default function: Rosenbrock â€” `(1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2`
- Default starting point: `xâ‚€ = [-1.2, 1.0]`
- Try both methods and compare convergence speed and number of iterations!